---
title: "NEH recommendation models evaluation"
author: "Lucas Marcondes Pavelski"
date: "Februrary 3, 2021"
output:
  html_document: default
  pdf_document:
    keep_tex: yes
  word_document: default
---

## Instances generation

```{r setup, include=FALSE}
library(FlowshopSolveR)
library(tidyverse)
library(tidymodels)
library(metaOpt)
library(vip)
library(PMCMRplus)
library(ggallin)
library(rpart.plot)


knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

MODEL_FOLDER <- here("data", "models")
TASK_NAME <- "NEH_recommendation"
TASK_FOLDER <- file.path(MODEL_FOLDER, TASK_NAME)

read_exp_dt <- function(param, model_name, exp_name, dt_name) {
  path <- file.path(TASK_FOLDER, param, paste0(model_name, ',', exp_name), dt_name)
  if (file.exists(path)) {
    readRDS(path)
  } else {
    NULL
  }
}

getParamName <- function(param) {
  case_when(
    param == "NEH.Init.NEH.Ratio" ~ 'IOR',
    param == "NEH.Init.NEH.First.Priority" ~ 'IOI', 
    param == "NEH.Init.NEH.First.PriorityWeighted" ~ 'IOW', 
    param == "NEH.Init.NEH.First.PriorityOrder" ~ 'IOS', 
    param == "NEH.Init.NEH.Priority" ~ 'NOI', 
    param == "NEH.Init.NEH.PriorityOrder" ~ 'NOS', 
    param == "NEH.Init.NEH.PriorityWeighted" ~ 'NOW', 
    param == "NEH.Init.NEH.Insertion" ~ 'NTB'
  )
}
```

This sections show the processing times values for the generated instances.

The plot below shows the processing times density plot for each of the generated distributions.

```{r instances_distributions, cache = T, fig.asp=0.33, echo=F}
set.seed(654)
no_samples <- 200
no_samples2 <- 40
tibble(
  Distribution = factor(c(rep('uniform', no_samples*no_samples2), 
                   rep('erlang', no_samples*no_samples2), 
                   rep('exponential', no_samples*no_samples2)), 
                   levels = c('uniform', 'exponential', 'erlang')),
  dt = c(
    as.integer(generate_test_instance(no_samples, no_samples2, 'uniform', corr = 'random', corv = 0)),
    as.integer(generate_test_instance(no_samples, no_samples2, 'erlang', corr = 'random', corv = 0)),
    as.integer(generate_test_instance(no_samples, no_samples2, 'exponential', corr = 'random', corv = 0))
  )
) %>%
  
  filter(dt > 1, dt < 200) %>%
  ggplot() +
  facet_wrap(~Distribution) +
  geom_density(aes(x = dt, fill = Distribution)) +
  theme_bw() +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(), 
        axis.title.y = element_blank())

```

The plot below shows the processing times for two machines on instances with different correlation values.

```{r instances_correlations, cache = T, fig.asp=0.69, echo=F}
set.seed(654)
no_samples <- 2000
no_samples2 <- 2

corrPts <- function(Distribution, Correlation) {
  pts <- generate_test_instance(
    no_samples, 
    no_samples2,
    Distribution,
    Correlation,
    ifelse(Correlation == 'random', 0, 0.95)
  )
  tibble(
    m1 = pts[,1],
    m2 = pts[,2]
  )
}

crossing(
  Distribution = c('uniform', 'erlang', 'exponential'),
  Correlation = c('random', 'job-correlated')
) %>%
  mutate(data = pmap(., corrPts)) %>%
  unnest(data) %>%
  ggplot() +
  facet_wrap(Correlation~Distribution) +
  geom_point(aes(x = m1, y = m2))

```

## Algorithm

Algoritm parameter details:

```{r algorithm, echo=F}
algorithm <- get_algorithm("NEH")

tibble(
  param = getParamName(algorithm@parameters$names),
  type = algorithm@parameters$types,
  values = map_chr(algorithm@parameters$names, 
                   ~paste(algorithm@parameters$domain[[.x]], collapse = ' '))
) %>%
  filter(!is.na(param)) %>%
  knitr::kable()
```

## Features

```{r load-features, include=F, cache=T}
features_dt <- read_csv(here("data/models/NEH_recommendation/features.csv"))

feature_names <- features_dt %>%
  select(-name, -instance, -type, -objective, -no_machines, -no_jobs) %>%
  colnames()
```

The plots below show the histograms for all features colored by: problem type, objective, number of jobs, number of machines and correlation type (for generated instances).

```{r features-by-type, echo=F, fig.asp=3.5}
features_dt %>%
  select(-name) %>%
  pivot_longer(all_of(feature_names), names_repair = "minimal") %>%
  ggplot(aes(x = value, fill = factor(type))) +
  facet_wrap(~name, scales = 'free', ncol = 3) +
  scale_fill_viridis_d() +
  geom_histogram() +
  theme_bw() +
  theme(legend.position = 'bottom') +
  ggtitle("Features distributions colored by the problem type")
```


```{r features-by-obj, echo=F, fig.asp=3.5}
features_dt %>%
  select(-name) %>%
  pivot_longer(all_of(feature_names), names_repair = "minimal") %>%
  ggplot(aes(x = value, fill = factor(objective))) +
  facet_wrap(~name, scales = 'free', ncol = 3) +
  scale_fill_viridis_d() +
  geom_histogram() +
  theme_bw() +
  theme(legend.position = 'bottom') +
  ggtitle("Features distributions colored by the objective")
```

```{r features-by-jobs, echo=F, fig.asp=3.5}
features_dt %>%
  select(-name) %>%
  pivot_longer(all_of(feature_names), names_repair = "minimal") %>%
  ggplot(aes(x = value, fill = factor(no_jobs))) +
  facet_wrap(~name, scales = 'free', ncol = 3) +
  scale_fill_viridis_d() +
  geom_histogram() +
  theme_bw() +
  theme(legend.position = 'bottom') +
  ggtitle("Features distributions colored by the number of jobs")
```

```{r features-by-machs, echo=F, fig.asp=3.5}
features_dt %>%
  select(-name) %>%
  pivot_longer(all_of(feature_names), names_repair = "minimal") %>%
  ggplot(aes(x = value, fill = factor(no_machines))) +
  facet_wrap(~name, scales = 'free', ncol = 3) +
  scale_fill_viridis_d() +
  geom_histogram() +
  theme_bw() +
  theme(legend.position = 'bottom') +
  ggtitle("Features distributions colored by the number of machines")
```

```{r features-by-corr, echo=F, fig.asp=3.5}
features_dt %>%
  select(-name) %>%
  pivot_longer(all_of(feature_names), names_repair = "minimal") %>%
  mutate(corr = str_replace_all(str_extract(instance, "_[a-z-]*_"), '_', '')) %>%
  filter(!is.na(corr)) %>%
  ggplot(aes(x = value, fill = factor(corr))) +
  facet_wrap(~name, scales = 'free', ncol = 3) +
  scale_fill_viridis_d() +
  geom_histogram() +
  theme_bw() +
  theme(legend.position = 'bottom') +
  ggtitle("Features distributions colored by correlation type on generated instances")
```

## Best parameters

```{r load-params, include=F, cache=T}
problem_cols <- c('obj', 'type', 'sc', 'bud', 'prob', 'dist', 'corr', 'no_jobs', 'no_machines')

params_dt <- read_csv(here("data/models/NEH_recommendation/outputs.csv")) %>% 
  separate(name, problem_cols, sep = ',') %>%
  mutate(no_jobs = factor(as.integer(no_jobs))) %>%
  mutate(no_machines = factor(as.integer(no_machines)))

params_cols <- c("NEH.Init.NEH.Ratio", "NEH.Init.NEH.First.Priority", 
"NEH.Init.NEH.First.PriorityWeighted", "NEH.Init.NEH.First.PriorityOrder", 
"NEH.Init.NEH.Priority", "NEH.Init.NEH.PriorityOrder", "NEH.Init.NEH.PriorityWeighted", 
"NEH.Init.NEH.Insertion")

param_histogram <- function(param, fill, title, stat) {
  params_dt %>%
    select(all_of(c(problem_cols, param))) %>%
    filter(across(all_of(param), ~!is.na(.x))) %>%
    ggplot(aes_string(x = param, fill = fill)) +
    scale_fill_viridis_d() +
    geom_histogram(stat = stat) +
    theme_bw() +
    theme(legend.position = 'bottom') +
    ggtitle(paste(getParamName(param), title))
}
```

The sections below show the distributions of the best values for each parameters colored by the objective, problem type, number of jobs and number of machines.

### Parameter values by objective

```{r params-by-obj, echo=F}
for (param in params_cols) {
  stat <- ifelse(param == "NEH.Init.NEH.Ratio", "bin", "count")
  show(param_histogram(param, "obj", "by objective", stat))
}
```

### Parameter values by problem type

```{r params-by-type, echo=F}
for (param in params_cols) {
  stat <- ifelse(param == "NEH.Init.NEH.Ratio", "bin", "count")
  show(param_histogram(param, "type", "by problem type", stat))
}
```

### Parameter values by number of jobs

```{r params-by-jobs, echo=F}
for (param in params_cols) {
  stat <- ifelse(param == "NEH.Init.NEH.Ratio", "bin", "count")
  show(param_histogram(param, "no_jobs", "by number of jobs", stat))
}
```

### Parameter values by number of machines

```{r params-by-machs, echo=F}
for (param in params_cols) {
  stat <- ifelse(param == "NEH.Init.NEH.Ratio", "bin", "count")
  show(param_histogram(param, "no_machines", "by number of machines", stat))
}
```

## Recommendation models

```{r load-models-dt, include=FALSE}

params <- algorithm@parameters$names[2:length(algorithm@parameters$names)]

model_names <- c(
  'decision_tree',
  'rand_forest'
)

exp_names <- c(
  'instance-based',
  'instance-based-dependencies'
)

all_exps <- expand.grid(
  param = params,
  model_name = model_names,
  exp_name = exp_names
)

test_perfs <- all_exps %>%
  mutate(perf = pmap(., read_exp_dt, dt_name = 'test_perf')) %>%
  unnest(perf)

all_perfs <- bind_rows(
  mutate(test_perfs, set = 'test')
) %>%
  mutate(
    model_name = case_when(
      model_name == 'decision_tree' ~ 'DT',
      model_name == 'rand_forest' ~ 'RF'
    )
  )


plot_metric <- function(pmetric) {
  all_perfs %>%
    filter(.metric == pmetric) %>%
    ggplot() +
    facet_wrap(~param, ncol = 3) +
    geom_col(aes(x = model_name, y = .estimate, fill = exp_name), 
             position = 'dodge') +
    coord_flip() +
    theme_bw() +
    theme(legend.position = 'bottom', axis.title = element_blank())
}
```


Accuracy for each recommendation model and parameter:


```{r ml-results-accuracy, fig.asp=0.45, echo=FALSE}
all_perfs %>%
  filter(.metric == 'accuracy', exp_name != 'ablation') %>%
  mutate(
    exp_name = case_when(
      exp_name == 'instance-based' ~ 'No dependencies',
      exp_name == 'instance-based-dependencies' ~ 'With dependencies',
      T ~ as.character(exp_name)
    ),
    param = factor(getParamName(param), c(
      'IOR', 'IOI', 'IOW', 'IOS', 'NOI', 'NOS', 'NOW', 'NTB' 
    ))
  ) %>%
  ggplot() +
  facet_wrap(~param, ncol = 4) +
  geom_col(aes(x = paste(model_name), y = .estimate, fill = exp_name), 
           position = 'dodge') +
  coord_flip() +
  theme_bw() +
  theme(legend.position = 'bottom', axis.title = element_blank(), legend.title = element_blank())
```

F-score for each recommendation model and parameter:

```{r ml-results-f-measure, fig.asp=0.45, echo=FALSE}
 all_perfs %>%
    filter(.metric == 'f_meas', exp_name != 'ablation') %>%
    mutate(
      exp_name = case_when(
        exp_name == 'instance-based' ~ 'No dependencies',
        exp_name == 'instance-based-dependencies' ~ 'With dependencies',
        T ~ as.character(exp_name)
      ),
      param = factor(getParamName(param), c(
        'IOR', 'IOI', 'IOW', 'IOS', 'NOI', 'NOS', 'NOW', 'NTB' 
      ))
    ) %>%
    ggplot() +
    facet_wrap(~param, ncol = 4) +
    geom_col(aes(x = paste(model_name), y = .estimate, fill = exp_name), 
             position = 'dodge') +
    coord_flip() +
    theme_bw() +
    theme(legend.position = 'bottom', axis.title = element_blank(), legend.title = element_blank())
```


### Decision tree models

The plots below show the decision trees generated for each parameter task.

```{r dt-models, echo=FALSE, warning=F}
for (param in params) {
  model <- read_exp_dt(
    param = param,
    model_name = 'decision_tree',
    exp_name = 'instance-based',
    dt_name = 'model'
  )
  rpart.plot(model)
  title(paste("DT recommendation model for", getParamName(param)))
}
```

Bellow are all the decision trees for the recommendation tasks considering parameter dependencies:

```{r dt-models-with-dependencies, echo=FALSE, warning=F}
for (param in params) {
  model <- read_exp_dt(
    param = param,
    model_name = 'decision_tree',
    exp_name = 'instance-based-dependencies',
    dt_name = 'model'
  )
  rpart.plot(model)
  title(paste("DT recommendation model for", getParamName(param), "with dependencies"))
}
```

### Decision trees variable importance

The bar plots below show, for each parameter, the variable importance for the decision tree model without dependencies: 

```{r, fig.asp=.5, echo=FALSE, results='asis', warning=F}
for (param in params) {
  model <- read_exp_dt(
    param = param,
    model_name = 'decision_tree',
    exp_name = 'instance-based',
    dt_name = 'model'
  )
  
  plt <- vip(model) + ggtitle(paste("DT variable importance for param", getParamName(param), "recommendation"))
  show(plt)
}
```
For the models including parameter dependencies, the variances importance are shown below: 

```{r, fig.asp=.5, echo=FALSE, results='asis', warning=F}
for (param in params) {
  model <- read_exp_dt(
    param = param,
    model_name = 'decision_tree',
    exp_name = 'instance-based-dependencies',
    dt_name = 'model'
  )
  
  plt <- vip(model) + ggtitle(paste("DT variable importance for param", getParamName(param), "recommendation "))
  show(plt)
}
```

### Models optimization performance

```{r results='asis', include=F, cache=T}
best_instance <- readRDS(file.path(TASK_FOLDER, 'results', 'none,instance_best.rda')) %>%
  unnest(problem) %>%
  rename(best_fitness = fitness)

# test_problems_df()

experiments <- tribble(
  ~ml_model, ~strat,
  'decision_tree', 'instance-based',
  'decision_tree', 'instance-based-dependencies',
  'rand_forest', 'instance-based',
  'rand_forest', 'instance-based-dependencies',
  'none', 'default',
  'none', 'global_best',
  'none', 'random',
) %>%
  mutate(results = pmap(., function(ml_model, strat) file.path(TASK_FOLDER,
                                        'results',
                                        paste0(ml_model, ',', strat, '.rda')))) %>%
  mutate(results = map(results, function(res) {
    print(res)
    readRDS(res)
  })) %>%
  unnest(results) %>%
  unnest(problem) %>%
  select(-config) %>%
  left_join(best_instance) %>%
  mutate(model_strat = factor(
    paste(ml_model, strat),
    levels = c(
      'none random',
      'none default',
      'none global_best',
      'decision_tree instance-based',
      'decision_tree instance-based-dependencies',
      'rand_forest instance-based',
      'rand_forest instance-based-dependencies'
    ),
    labels = c(
      'Random',
      'Standard NEH',
      'Global-best NEH',
      'DT',
      'DT+Dependencies',
      'RF',
      'RF+Dependencies'
    )
  )) %>%
  mutate(rp = 100 * (fitness - best_fitness) / best_fitness)

```

The optimization performance considered the average relative performance $(perf - best\_perf) / best\_perf$.

Th quantiles for the performance values for each model are shown in the table below.

```{r perf-quantiles, echo=F}
experiments %>%
  group_by(model_strat) %>%
  summarize(
    q00 = quantile(rp, 0.00),
    q25 = quantile(rp, 0.25),
    q50 = quantile(rp, 0.50),
    q75 = quantile(rp, 0.75),
    q100  = quantile(rp, 1.00),
  )  %>% knitr::kable()
```

Below is the violin plot for each model performance.

```{r strategies-comparison, echo=F}
experiments %>%
  ggplot() +
  geom_violin(aes(x = model_strat, fill = model_strat, y = rp)) +
  scale_y_continuous(trans = pseudolog10_trans) +
  scale_fill_viridis_d() +
  theme_bw() +
  theme(axis.title = element_blank(), axis.text.x = element_blank()) +
  labs(fill = "Strategy")
```

And filtering the random choice performance:

```{r strategies-comparison-without-random, echo=F}
experiments %>%
  filter(strat != 'random') %>%
  ggplot() +
  geom_violin(aes(x = model_strat, fill = model_strat, y = rp)) +
  scale_y_continuous(trans = pseudolog10_trans) +
  scale_fill_viridis_d() +
  theme_bw() +
  theme(axis.title = element_blank(), axis.text.x = element_blank()) +
  labs(fill = "Strategy")
```

The Friedman test was used on the optimization performance data considering each instance as a block. The table below shows the test p-values adjusted with Nemenyi post-hoc:

```{r stat-all-pairs, echo=F}
stat_data <- experiments %>%
  mutate(
    block = paste(model, instance),
    group = model_strat,
    y = rp
  ) %>%
  select(block, group, y)

frdAllPairsNemenyiTest(stat_data$y, stat_data$group, stat_data$block)$p.value %>% knitr::kable()
```
The following test compares RF with dependencies model (the one with the best performance) against all other models considering the optimization performance. It uses the many-to-one Friedman test with Demsar post-hoc.

```{r stat-one-to-many, echo=FALSE}
stat_data <- experiments %>%
  mutate(
    block = paste(model, instance),
    group = factor(paste(ml_model, strat),
      levels = c(
        'rand_forest instance-based-dependencies',
        'none random',
        'none default',
        'none global_best',
        'decision_tree instance-based',
        'decision_tree instance-based-dependencies',
        'rand_forest instance-based'
      ),
      labels = c(
        'RF+Dependencies',
        'Random',
        'Standard NEH',
        'Global-best NEH',
        'DT',
        'DT+Dependencies',
        'RF'
      )
    ),
    y = rp
  ) %>%
  select(block, group, y)

frdManyOneDemsarTest(stat_data$y, stat_data$group, stat_data$block)$p.value %>% knitr::kable()
```




